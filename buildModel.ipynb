{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # for torch.nn.Module, the parent object foe PyTorch models.\n",
    "import torch.nn.functional as F # for thr activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avater](https://pytorch.org/tutorials/_images/mnist.png)\n",
    "\n",
    "Above is a diagram of LeNet-5, one of the earliest convolutional neural nets, and one of the drivers of the explosion in Deep Learning. It was built to read small images of handwritten numbers (the MNIST dataset), and correctly classify which digit was represented in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: a simple neural network classify input images as 10 categories of numbers\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel(black and white), 6 output channels,  3*3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16*6*6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max pooling over a (2,2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimentions except the batch dimention\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Image batch shape:\n",
      "torch.Size([16, 1, 32, 32])\n",
      "\n",
      "Raw output:\n",
      "tensor([[ 0.0399, -0.1280, -0.0132, -0.0539,  0.0682, -0.0550, -0.0191, -0.0210,\n",
      "          0.0761,  0.0971],\n",
      "        [ 0.0389, -0.1278, -0.0143, -0.0554,  0.0657, -0.0562, -0.0221, -0.0203,\n",
      "          0.0799,  0.0930],\n",
      "        [ 0.0396, -0.1300, -0.0192, -0.0587,  0.0706, -0.0545, -0.0157, -0.0131,\n",
      "          0.0786,  0.0955],\n",
      "        [ 0.0369, -0.1253, -0.0121, -0.0635,  0.0721, -0.0596, -0.0249, -0.0212,\n",
      "          0.0788,  0.0914],\n",
      "        [ 0.0389, -0.1252, -0.0111, -0.0618,  0.0725, -0.0548, -0.0213, -0.0221,\n",
      "          0.0811,  0.0963],\n",
      "        [ 0.0368, -0.1291, -0.0109, -0.0519,  0.0627, -0.0585, -0.0230, -0.0216,\n",
      "          0.0781,  0.0963],\n",
      "        [ 0.0370, -0.1301, -0.0159, -0.0548,  0.0682, -0.0554, -0.0113, -0.0205,\n",
      "          0.0696,  0.0954],\n",
      "        [ 0.0402, -0.1263, -0.0150, -0.0580,  0.0626, -0.0552, -0.0225, -0.0195,\n",
      "          0.0799,  0.0922],\n",
      "        [ 0.0442, -0.1296, -0.0157, -0.0576,  0.0709, -0.0525, -0.0222, -0.0141,\n",
      "          0.0815,  0.0962],\n",
      "        [ 0.0428, -0.1282, -0.0137, -0.0598,  0.0664, -0.0505, -0.0241, -0.0185,\n",
      "          0.0825,  0.0922],\n",
      "        [ 0.0393, -0.1282, -0.0168, -0.0580,  0.0712, -0.0563, -0.0207, -0.0186,\n",
      "          0.0760,  0.0912],\n",
      "        [ 0.0387, -0.1284, -0.0143, -0.0564,  0.0666, -0.0550, -0.0190, -0.0231,\n",
      "          0.0769,  0.0911],\n",
      "        [ 0.0348, -0.1288, -0.0189, -0.0627,  0.0695, -0.0591, -0.0195, -0.0219,\n",
      "          0.0789,  0.0926],\n",
      "        [ 0.0358, -0.1259, -0.0176, -0.0574,  0.0699, -0.0553, -0.0242, -0.0205,\n",
      "          0.0776,  0.0913],\n",
      "        [ 0.0394, -0.1296, -0.0144, -0.0599,  0.0733, -0.0550, -0.0214, -0.0209,\n",
      "          0.0785,  0.0911],\n",
      "        [ 0.0371, -0.1265, -0.0148, -0.0563,  0.0643, -0.0616, -0.0193, -0.0221,\n",
      "          0.0802,  0.0935]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net) # what does the object tell us itself?\n",
    "\n",
    "# stand-in for a black & white 32*32 image\n",
    "# the first \"16\" stands for the number of batches\n",
    "# we have 16 input and finally get 16 output\n",
    "input = torch.rand(16,1,32,32) \n",
    "\n",
    "print(\"\\nImage batch shape:\")\n",
    "print(input.shape)\n",
    "\n",
    "output = net(input) # we don't call forward() directly\n",
    "# output = net.forward(input)\n",
    "print(\"\\nRaw output:\")\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25ce29bed60a3e0bbd870304a4fd2eae25598c1eec88a2bcbbe85a0b870cbd3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
